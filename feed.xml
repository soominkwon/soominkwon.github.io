<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://soominkwon.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://soominkwon.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-11-23T20:20:08+00:00</updated><id>https://soominkwon.github.io/feed.xml</id><title type="html">blank</title><subtitle></subtitle><entry><title type="html">Gaudi-GBLR: Differentiable, and Generalized Block Low-Rank Matrix Format for Efficient DNNs</title><link href="https://soominkwon.github.io/blog/2024/gaudi-gblr/" rel="alternate" type="text/html" title="Gaudi-GBLR: Differentiable, and Generalized Block Low-Rank Matrix Format for Efficient DNNs"/><published>2024-01-22T00:00:00+00:00</published><updated>2024-01-22T00:00:00+00:00</updated><id>https://soominkwon.github.io/blog/2024/gaudi-gblr</id><content type="html" xml:base="https://soominkwon.github.io/blog/2024/gaudi-gblr/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Deep Neural Networks (DNNs) are getting bigger and bigger. A way to reduce the complexity of the DNNs is to use the <em>structured</em> weight matrices, like Low-Rank or Sparse matrices.</p> <p>This work introduces a <strong>Gaudi-GBLR</strong> matrix, which is a (G)eneralized (B)lock (L)ow-(R)ank Matrix with (Gau)ssian-(Di)richlet Function.</p> <ul> <li><em>Expressiveness.</em> <strong>Gaudi-GBLR</strong> includes other structured matrices such as Low-Rank, Block-Low-Rank, and Block-Sparse matrices.</li> <li><em>Differentiability.</em> The structure of the <strong>Gaudi-GBLR</strong> is <strong>differentiable</strong>! The efficient structure of the weight matrix is learned from data. So the efficient DNN can be learned from scratch!</li> <li><em>Layer-wise Optimization.</em> The structure (and the complexity) is optimized in a layer-wise manner. The less important, the less complexity.</li> </ul> <h3 id="generalized-block-low-rank-gblr-matrix">Generalized Block Low-Rank (GBLR) Matrix</h3> <p>A <strong>GBLR</strong> matrix is a generalized version of the Block Low-Rank (BLR) matrix. Unlike the BLR structure, a <strong>GBLR</strong> matrix is composed of multiple overlapping low-rank blocks. Notably, the <strong>GBLR</strong> structure includes multiple important efficient matrix structures. In our <a href="https://openreview.net/forum?id=pAVJKp3Dvn">paper</a>, we analyzed that the <strong>GBLR</strong> format contains <em>Low-Rank, Block-Sparse, Block-Low-Rank</em> matrices of the same complexity for the matrix-vector product.</p> <div align="center"> <img src="https://changwoolee.github.io/assets/img/projects/gaudi-gblr/block_matrices.drawio.webp" alt="gblr" width="400"/> </div> <p>The key idea is to learn the location and the area of each block from data. Once they are found, the matrix-vector product can be done faster on the specialized hardware. We left demonstrating the actual speedup as future work.</p> <div align="center"> <img src="https://changwoolee.github.io/assets/img/projects/gaudi-gblr/GBLR-detailed.webp" alt="gblr-detailed" width="800"/> </div> <h3 id="gaussian-dirichlet-gaudi-function-for-differentiability">Gaussian-Dirichlet (Gaudi) Function for Differentiability</h3> <p>Unfortunately, optimizing the structural (location and area) parameters of the GBLR matrix is not easy. The parameters are defined in the discrete space, and non-differentiable.</p> <p>Here, we circumvent the problem by defining the structural parameter in the <em>frequency</em> domain. The location and the width and height of the low-rank block appear <strong>explicitly</strong> and <strong>differentiably</strong> in the form of the <strong><a href="https://en.wikipedia.org/wiki/Dirichlet_kernel">Dirichlet</a></strong> function, which is the DFT pair of the Boxcar function. By taking a Gaussian filter for the numerical stability, we obtain a <strong>Gaussian-Dirichlet (Gaudi)</strong> function to indicate the position of the low-rank block.</p> <div align="center"> <img src="https://changwoolee.github.io/assets/img/projects/gaudi-gblr/Gaudi.webp" alt="gaudi" width="400"/> </div> <h3 id="layer-wise-optimization">Layer-wise Optimization</h3> <p>Intuitively, we donâ€™t think all layers are equally important. Some layers might contribute less than others, which indicates that less important layers can be compressed more.</p> <p>Unfortunately, it has been very time-consuming to allocate different number of computations for each layer since the search space is discrete and the problem is NP-hard.</p> <p>In contrast, with the <strong>GBLR</strong> format, the <strong>layer-wise structural parameter optimization</strong> can be easily done because we can update the structural parameters by Stochastic Gradient Descent (SGD).</p> <p>The figure below illustrates the learned Gaudi-GBLR weight matrices of the ViT-Base model trained on ImageNet. The brighter, the more overlapping low-rank blocks. Each weight has different rank and structure, which are found during the training process by SGD.</p> <div align="center"> <img src="https://changwoolee.github.io/assets/img/projects/gaudi-gblr/mask_patterns.webp" alt="gaudi" width="400"/> </div> <h3 id="bibtex">BibTeX</h3> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@inproceedings{lee2023differentiable,
  title={Differentiable Learning of Generalized Structured Matrices for Efficient Deep Neural Networks},
  author={Lee, Changwoo and Kim, Hun-Seok},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024},
  url={https://openreview.net/forum?id=pAVJKp3Dvn}
}
</code></pre></div></div>]]></content><author><name>Changwoo Lee</name></author><category term="Research"/><category term="LinearAlgebra"/><category term="MachineLearning"/><category term="ICLR2024"/><summary type="html"><![CDATA[A project page for the Gaudi-GBLR paper.]]></summary></entry></feed>